{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take home points by Jun Pan\n",
    "\n",
    "Titel: Music Recommendations at Scale with Spark \n",
    "    YouTube URL: http://www.youtube.com/watch?v=3LBgiFch4_g\n",
    "          \n",
    "Speaker: Christopher Johnson is Spotify Employee  \n",
    "      \n",
    "Background information: Spotify is a music streaming service which at the time of lecture, contained a huge catalog of over 40 million songs.  \n",
    "\n",
    "Spotify uses collaborative filtering recommendation engine.\n",
    "Collaborative Filtering is commonly used for recommender systems. It is a technique to fill in the missing entries of a user-item association matrix. MLlib (MLlib is Spark's scalable machine learning library consisting of common learning algorithms and utilities) currently supports model-based collaborative filtering, in which users and products are described by a small set of latent factors that can be used to predict missing entries. MLlib uses the alternating least squares (ALS) algorithm to learn these latent factors.\n",
    "    \n",
    "  \n",
    "Implicit matrix factorization is what Spotify uses to reduce the overall size of matrix.\n",
    "The standard approach to matrix factorization based collaborative filtering treats the entries in the user-item matrix as explicit preferences given by the user to the item. It is common in many real-world use cases which only have access to implicit feedback (e.g. views, clicks, purchases, likes, shares etc.). The approach used in MLlib to deal with such data is taken from Collaborative Filtering for Implicit Feedback Datasets. Essentially instead of trying to model the matrix of ratings directly, this approach treats the data as a combination of binary preferences and confidence values.\n",
    "    \n",
    "Switch using \"spark\" from Hadoop helps Spotify to reduce computation time significantly. Spark lets them read the rating matrix from disk ONCE and keep in the rest in RAM. \n",
    "\n",
    "For example: \n",
    "      First attempt: \"broadcast everything\" (Hadoop)\n",
    "        Run time: 10 hours\n",
    "      Second attempt: \"full gridify\" (Spark)\n",
    "        Run time: 3.5 hours\n",
    "      Third attempt: \"half gridify\"  (Spark)\n",
    "        Run time: 1.5 hours\n",
    "    \n",
    "Apache Spark is 30x faster than Hadoop, because the huge difference between RAM and Disk.  Spark is a cluster-computing framework which competes more with MapReduce than with the entire Hadoop ecosystem. For example, Spark doesn't have its own distributed filesystem. Spark uses memory and can use disk for processing, whereas MapReduce is strictly disk-based. Therefore, Spark is simpler and faster than Mapreduce for the usual Machine learning and Data Analytics applications. In Full gridify scheme, a lot of intermediate data is sent over the wire for each iteration and a large I/O overhead compared to the half gridify scheme, but has a potential for requiring less local memory comapred to half gridify.  For a half gridify scheme, ratings get cached and never shuffled. Once item vectors are joined with ratings partitions each partition has enough information to solve optimal user and vectors without any additional shuffling.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
